{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Jyo Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKX-69P97-QK"
      },
      "source": [
        "Google Connect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lltdd_QdoHzb",
        "outputId": "77aec6d6-1f56-4e61-c34e-1b53a249d30f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-Rc5RiM8A1p"
      },
      "source": [
        "Import library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvSXMo3goJ0i"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "# %matplotlib ipympl\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn import model_selection\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHwdPaKr8G41"
      },
      "source": [
        "Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7HHnia9oQio"
      },
      "source": [
        "# Google Colab\n",
        "train_all = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Advance Data Analysis/Assignment 2/Practice/FIT5149_A2_data/train_data_withlabels.csv')\n",
        "test_all = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Advance Data Analysis/Assignment 2/Practice/FIT5149_A2_data/test_data_nolabels.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq6a60SLoRmW"
      },
      "source": [
        "# train_all.head()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vkgU6DAoPNw"
      },
      "source": [
        "# test_all.head()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82jRD3yg8IZe"
      },
      "source": [
        "Needed Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DikHL4UppxUe"
      },
      "source": [
        "day_of_week_train = pd.get_dummies(train_all.dayofweek)\n",
        "X2_train = train_all.drop('dayofweek', 1)\n",
        "X3_train = pd.concat([X2_train, day_of_week_train], axis=1)\n",
        "\n",
        "# X3_train.head()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtX5YteKVjUI"
      },
      "source": [
        "# print(train_all.shape)\n",
        "# print(X3_train.shape)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktkbkyx-pztU"
      },
      "source": [
        "day_of_week_test = pd.get_dummies(test_all.dayofweek)\n",
        "X2_test = test_all.drop('dayofweek', 1)\n",
        "X3_test = pd.concat([X2_test, day_of_week_test], axis=1)\n",
        "\n",
        "# X3_test.head()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k716KuzV6zz"
      },
      "source": [
        "# print(test_all.shape)\n",
        "# print(X3_test.shape)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqflRbszoT_j"
      },
      "source": [
        "cols = ['load', 'hourofday',\t'dif',\t'absdif',\t'max', 'var', 'entropy',\t'nonlinear',\t'hurst', 'Fri',\t'Mon',\t'Sat',\t'Sun',\t'Thu', 'Tue',\t'Wed']\n",
        "target = ['ac',\t'ev',\t'oven',\t'wash',\t'dryer']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sehYHHgBoYQD"
      },
      "source": [
        "X_train_o = X3_train[cols]\n",
        "Y_train_o = X3_train[target]\n",
        "X_test_o = X3_test[cols]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QbcI-KSCbhs"
      },
      "source": [
        "# X_train.head()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEJDVlajCf4k"
      },
      "source": [
        "# Y_train.head()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKUyoZ_YVX4E"
      },
      "source": [
        "# X_test"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HByT655uUaw1"
      },
      "source": [
        "# print(X_train.shape)\n",
        "# print(X_train.head()\n",
        "# .shape)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOBGzSLICoiP"
      },
      "source": [
        "EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2TFzYE8Cn_8"
      },
      "source": [
        "# # Price chart\n",
        "# fig, ax = plt.subplots(figsize=(16, 8))\n",
        "# plt.plot(train_all['Unnamed: 0'], train_all['load'], label='load', color='gold')\n",
        "# # plt.title('Gold ' + str(np.min(x_train['Date'])) + ' - ' + str(np.max(x_train['Date'])))\n",
        "# plt.legend(loc='upper left')\n",
        "# plt.grid()\n",
        "# plt.show()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBukppNVU5JD"
      },
      "source": [
        "# plt.figure(figsize=(12,10))\n",
        "# cor = X3_train.corr()\n",
        "# sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
        "# plt.show()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt24xQIdXtpl"
      },
      "source": [
        "# # check missing values in train data\n",
        "# X3_train.isnull().sum()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni27Rlt1Xy6g"
      },
      "source": [
        "# for i in target:\n",
        "#   print(i)\n",
        "  \n",
        "#   g = sns.countplot(train_all[i])\n",
        "#   g.set_xticklabels(['0','1'])\n",
        "#   plt.show()\n",
        "\n",
        "#   count_no_sub = len(train_all[train_all[i]==0])\n",
        "#   count_sub = len(train_all[train_all[i]==1])\n",
        "#   print(\"No of 0 :\", count_no_sub)\n",
        "#   print(\"No of 1 :\", count_sub, '\\n')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihkORT5uM2rh"
      },
      "source": [
        "# # class count\n",
        "# class_count_0, class_count_1 = X3_train['ac'].value_counts()\n",
        "      \n",
        "# # Separate class\n",
        "# class_0 = X3_train[X3_train['ac'] == 0]\n",
        "# class_1 = X3_train[X3_train['ac'] == 1]\n",
        "# # print the shape of the class\n",
        "# print('class 0:', class_0.shape)\n",
        "# print('class 1:', class_1.shape)\n",
        "\n",
        "# class_1_over = class_1.sample(class_count_0, replace=True)\n",
        "\n",
        "# test_over = pd.concat([class_1_over, class_0], axis=0)\n",
        "\n",
        "# print(\"total class of 1 and 0:\",test_over['ac'].value_counts())# plot the count after under-sampeling\n",
        "# test_over['ac'].value_counts().plot(kind='bar', title='count (target)')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llnn-86Z8L4u"
      },
      "source": [
        "1) Train Test Split (Random Split 70:30)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXWtqcaxUV4F"
      },
      "source": [
        "x_train_o, x_valid_o, y_train_o, y_valid_o = train_test_split(X_train_o, Y_train_o)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SicKpd04tQ60"
      },
      "source": [
        "# x_train.head()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhaK9PHYtLBs"
      },
      "source": [
        "# y_valid.head()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOe2FpbOtcAZ"
      },
      "source": [
        "# for i in target:\n",
        "#   print(i)\n",
        "#   count_no_sub = len(y_train[y_train[i]==0])\n",
        "#   count_sub = len(y_train[y_train[i]==1])\n",
        "#   print(\"No of 0 :\", count_no_sub)\n",
        "#   print(\"No of 1 :\", count_sub, '\\n')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-brn6je3sz9X"
      },
      "source": [
        "# for i in target:\n",
        "#   print(i)\n",
        "#   count_no_sub = len(y_valid[y_valid[i]==0])\n",
        "#   count_sub = len(y_valid[y_valid[i]==1])\n",
        "#   print(\"No of 0 :\", count_no_sub)\n",
        "#   print(\"No of 1 :\", count_sub, '\\n')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrQhwUQ0M-au"
      },
      "source": [
        "def random_over_sampeling(X3_train, targ):\n",
        "  # class count\n",
        "  class_count_0, class_count_1 = X3_train[targ].value_counts()\n",
        "\n",
        "  # Separate class\n",
        "  class_0 = X3_train[X3_train[targ] == 0]\n",
        "  class_1 = X3_train[X3_train[targ] == 1]\n",
        "\n",
        "  class_1_over = class_1.sample(class_count_0, replace=True)\n",
        "  test_over = pd.concat([class_1_over, class_0], axis=0) \n",
        "\n",
        "  X_train_rs = test_over[cols]\n",
        "  Y_train_rs = test_over[target] \n",
        "\n",
        "\n",
        "\n",
        "  return X_train_rs, Y_train_rs"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXFsU5JyvNe3"
      },
      "source": [
        "def model_description_prediction(model_, x_train, y_train, x_valid, y_valid, targ):\n",
        "  model_.fit(x_train, y_train[targ])\n",
        "  y_pred = model_.predict(x_valid)\n",
        "  species = np.array(y_valid[targ])\n",
        "  predictions = np.array(y_pred)\n",
        "  #K-fold cross validation\n",
        "  results = model_selection.cross_val_score(estimator=model_, X=x_train, y=y_train[targ], cv=kfold, scoring='f1')\n",
        "\n",
        "  print('For Target -->',targ)\n",
        "  print('Confusion matrix :\\n', confusion_matrix(species, predictions))\n",
        "  print('Classification report : ', classification_report(species, predictions))\n",
        "  print('F1 score :', round(f1_score(species, predictions), 5))\n",
        "  print('F1 score on K folds:', round(results.mean(), 5), 'Standard derivation :', round(results.std(), 5))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNAxSvMj2v3d"
      },
      "source": [
        "K fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBydRWl32vLO"
      },
      "source": [
        "kfold = model_selection.KFold(n_splits=3, random_state=42, shuffle=True)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do6m6N4khFrs"
      },
      "source": [
        "Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF2qSNoqhB5i"
      },
      "source": [
        "model_LR = LogisticRegression(solver='liblinear')\n",
        "model_DT = DecisionTreeClassifier()\n",
        "model_RF = RandomForestClassifier(n_estimators=100, bootstrap = True , max_features = 'sqrt')\n",
        "model_GB = GradientBoostingClassifier(n_estimators=20, learning_rate=1, max_features=2, max_depth=2, random_state=0)\n",
        "model_NB = GaussianNB()\n",
        "model_SGD = SGDClassifier(loss='modified_huber', shuffle=True, random_state=101)\n",
        "model_KNN = KNeighborsClassifier(n_neighbors=15)\n",
        "# model_SVC = svm.SVC(C=0.5, random_state=101)\n",
        "\n",
        "# models_all = [model_LR, model_DT, model_RF, model_GB, model_NB, model_SGD, model_KNN]\n",
        "models_all = [model_NB]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBE14rAIh6_L",
        "outputId": "8461955f-4208-4fc0-faa0-ef2ee7c4c0c0"
      },
      "source": [
        "for model in models_all:\n",
        "  print('\\n', model)\n",
        "\n",
        "  for targ in target:\n",
        "\n",
        "    if model in (model_LR, model_DT):\n",
        "      #Without Over-Sampling\n",
        "      print('\\n=> No-Sampling')\n",
        "      model_description_prediction(model, x_train_o ,y_train_o, x_valid_o, y_valid_o, targ)\n",
        "    else:\n",
        "      #Random Over-Sampeling\n",
        "      print('\\n==> Random-Over-Sampling')\n",
        "      \n",
        "      X_train_rs, Y_train_rs = random_over_sampeling(X3_train, targ)\n",
        "      x_train_rs, x_valid_rs, y_train_rs, y_valid_rs = train_test_split(X_train_rs, Y_train_rs)\n",
        "\n",
        "      model_description_prediction(model, x_train_rs, y_train_rs, x_valid_rs, y_valid_rs, targ)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " GaussianNB(priors=None, var_smoothing=1e-09)\n",
            "\n",
            "==> Random-Over-Sampling\n",
            "For Target --> ac\n",
            "Confusion matrix :\n",
            " [[75958  3144]\n",
            " [ 1387 77772]]\n",
            "Classification report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97     79102\n",
            "           1       0.96      0.98      0.97     79159\n",
            "\n",
            "    accuracy                           0.97    158261\n",
            "   macro avg       0.97      0.97      0.97    158261\n",
            "weighted avg       0.97      0.97      0.97    158261\n",
            "\n",
            "F1 score : 0.97169\n",
            "F1 score on K folds: 0.9719 Standard derivation : 0.00014\n",
            "\n",
            "==> Random-Over-Sampling\n",
            "For Target --> ev\n",
            "Confusion matrix :\n",
            " [[83621 20112]\n",
            " [20659 83301]]\n",
            "Classification report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.81      0.80    103733\n",
            "           1       0.81      0.80      0.80    103960\n",
            "\n",
            "    accuracy                           0.80    207693\n",
            "   macro avg       0.80      0.80      0.80    207693\n",
            "weighted avg       0.80      0.80      0.80    207693\n",
            "\n",
            "F1 score : 0.80339\n",
            "F1 score on K folds: 0.8035 Standard derivation : 0.00027\n",
            "\n",
            "==> Random-Over-Sampling\n",
            "For Target --> oven\n",
            "Confusion matrix :\n",
            " [[89466 13606]\n",
            " [12946 89864]]\n",
            "Classification report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87    103072\n",
            "           1       0.87      0.87      0.87    102810\n",
            "\n",
            "    accuracy                           0.87    205882\n",
            "   macro avg       0.87      0.87      0.87    205882\n",
            "weighted avg       0.87      0.87      0.87    205882\n",
            "\n",
            "F1 score : 0.87128\n",
            "F1 score on K folds: 0.87089 Standard derivation : 0.00039\n",
            "\n",
            "==> Random-Over-Sampling\n",
            "For Target --> wash\n",
            "Confusion matrix :\n",
            " [[49113 53331]\n",
            " [19475 82965]]\n",
            "Classification report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.48      0.57    102444\n",
            "           1       0.61      0.81      0.70    102440\n",
            "\n",
            "    accuracy                           0.64    204884\n",
            "   macro avg       0.66      0.64      0.63    204884\n",
            "weighted avg       0.66      0.64      0.63    204884\n",
            "\n",
            "F1 score : 0.69504\n",
            "F1 score on K folds: 0.69516 Standard derivation : 0.00108\n",
            "\n",
            "==> Random-Over-Sampling\n",
            "For Target --> dryer\n",
            "Confusion matrix :\n",
            " [[42745 58210]\n",
            " [20067 81050]]\n",
            "Classification report :                precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.42      0.52    100955\n",
            "           1       0.58      0.80      0.67    101117\n",
            "\n",
            "    accuracy                           0.61    202072\n",
            "   macro avg       0.63      0.61      0.60    202072\n",
            "weighted avg       0.63      0.61      0.60    202072\n",
            "\n",
            "F1 score : 0.67436\n",
            "F1 score on K folds: 0.67485 Standard derivation : 0.00094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOBL6m1pH5U8"
      },
      "source": [
        "Random Forest HyperTuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7C8kimWAEwQ"
      },
      "source": [
        "# from sklearn import model_selection\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# rfc = RandomForestClassifier()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvT7eb1xAGQn"
      },
      "source": [
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "# # number of trees in random forest\n",
        "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "\n",
        "# # number of features at every split\n",
        "# max_features = ['aut', 'sqrt']\n",
        "\n",
        "# # max depth\n",
        "# max_depth = [int(x) for x in np.linspace(100, 500, num = 11)]\n",
        "# max_depth.append(None)\n",
        "\n",
        "# # create random grid\n",
        "# random_grid = {\n",
        "#  'n_estimators': n_estimators,\n",
        "#  'max_features': max_features,\n",
        "#  'max_depth': max_depth\n",
        "#  }\n",
        "\n",
        "# # Random search of parameters\n",
        "# rfc_random = RandomizedSearchCV(estimator = rfc, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo2f1afSAdJt"
      },
      "source": [
        "# # Fit the model\n",
        "\n",
        "# rfc_random.fit(x_val, small_train['ac'])\n",
        "# print(rfc_random.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt5Ohno_Af2B"
      },
      "source": [
        "# rf_tuned_model = RandomForestRegressor(n_estimators=200, max_features='sqrt', max_depth=None, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5V2mqVR_iiZ"
      },
      "source": [
        "Hyper tune Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzdglQ86_iFN"
      },
      "source": [
        "# lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
        "\n",
        "\n",
        "# for learning_rate in lr_list:\n",
        "#   print(\"\\nLearning rate:\", learning_rate)\n",
        "#   gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n",
        "#   for i in target:\n",
        "#     print(\"Target:\", i)\n",
        "#     gb_clf.fit(x_val, train[i])\n",
        "#     y_pred = gb_clf.predict(x_val_test)\n",
        "#     predictions = np.array(y_pred)\n",
        "#     test_y = np.array(test[i])\n",
        "#     print(\"F1\", f1_score(test[i], np.rint(predictions), labels = 'binary')) #np.rint is rounding array to nearest int\n",
        "#     print(confusion_matrix(test_y, np.rint(predictions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fjT9N7J7vne"
      },
      "source": [
        "Predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "ZDPhaVcm8Rcp",
        "outputId": "a6943546-f238-48b9-f95e-ee5a154b1bb3"
      },
      "source": [
        "X_test = X3_test[cols]\n",
        "tuned_df = pd.DataFrame(columns=['ac','ev','oven','wash','dryer'])\n",
        "for targ in target:\n",
        "  model = models_all[0]\n",
        "  X_train_f, Y_train_f = random_over_sampeling(X3_train, targ)\n",
        "  model.fit(X_train_f, Y_train_f[targ])\n",
        "  y_pred = model.predict(X_test_o)\n",
        "  predictions = np.array(y_pred)\n",
        "  tuned_df[targ]=predictions\n",
        "\n",
        "tuned_df1 = tuned_df\n",
        "tuned_df1.insert(0, 'id', range(1, 1+len(tuned_df)))\n",
        "\n",
        "tuned_df1.to_csv('/content/drive/MyDrive/Colab Notebooks/Advance Data Analysis/Assignment 2/Practice/Kaggle_test_output_data/test_03.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-1e470030d357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mX_train_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_over_sampeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX3_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_o\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 383\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}